<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <title>OpenAI Realtime (WebRTC) - Toggle & Transcript & Save + Tools</title>
  <style>
    :root { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
    * { box-sizing: border-box; }
    body { margin: 0; display: grid; grid-template-columns: 1fr 420px; grid-template-rows: auto 1fr; height: 100vh; }
    header { grid-column: 1 / -1; padding: 10px 16px; background: #111827; color: #e5e7eb; display: flex; gap: 12px; align-items: center; }
    main { padding: 16px; overflow: auto; }
    aside { border-left: 1px solid #e5e7eb; display: grid; grid-template-rows: auto 1fr auto; min-width: 320px; }
    .controls { display: flex; gap: 8px; align-items: center; }
    button { padding: 8px 14px; font-size: 14px; cursor: pointer; border: 1px solid #ddd; border-radius: 8px; background: #fff; }
    button:disabled { opacity: .6; cursor: not-allowed; }
    .status { margin-left: auto; opacity: .9; font-size: 14px; }
    .panel-title { padding: 10px 12px; font-weight: 600; border-bottom: 1px solid #eee; background: #f8fafc; }
    .log-wrap { position: relative; }
    .log { overflow: auto; padding: 12px; background: #fafafa; border-top: 1px solid #eee; border-bottom: 1px solid #eee; height: 100%; }
    .msg { margin: 10px 0; border-radius: 8px; padding: 8px 10px; background: #fff; border: 1px solid #eee; }
    .role { font-weight: 700; font-size: 12px; opacity: .75; margin-bottom: 4px; }
    .user .role { color: #2563eb; }
    .assistant .role { color: #059669; }
    .event .role { color: #6b7280; }
    .text { white-space: pre-wrap; word-break: break-word; }
    .footer { padding: 10px; display: flex; gap: 8px; justify-content: space-between; align-items: center; }
    .hint { font-size: 12px; color: #6b7280; }
    .avatar-wrap { position: fixed; left: 16px; top: 50%; transform: translateY(-50%); width: 360px; height: 360px; border-radius: 9999px; overflow: hidden; border: 2px solid #e5e7eb; background: #000; z-index: 20; box-shadow: 0 10px 30px rgba(0,0,0,.25); pointer-events: none; }
    .avatar-wrap video { width: 100%; height: 100%; object-fit: cover; }
    .avatar-wrap.speaking { animation: avatarPulse 1.4s infinite; }
    @keyframes avatarPulse { 0% { box-shadow: 0 0 0 0 rgba(99,102,241,.6); } 70% { box-shadow: 0 0 0 28px rgba(99,102,241,0); } 100% { box-shadow: 0 0 0 0 rgba(99,102,241,0); } }
    @media (max-width: 1200px) { .avatar-wrap { width: 260px; height: 260px; } }
  </style>
</head>
<body>
  <header>
    <div class="controls">
      <button id="toggle">▶ Start</button>
      <button id="clear" title="ログを消去">🧹 Clear</button>
      <button id="export" title="JSONで保存">💾 Export JSON</button>
    </div>
    <div class="status" id="status">Idle</div>
  </header>

  <main>
    <h2>Realtime Voice + Function Calling Demo</h2>
    <p>
      「▶ Start」で会話開始／「■ Stop」で停止。<br/>
      右側にアシスタントの返答が <b>字幕としてリアルタイム表示</b> され、確定後はログに保存されます。<br/>
      <b>試すなら：</b>「東京の天気を教えて」「大阪は華氏で」など。
    </p>
    <ul>
      <li>ログは <code>localStorage</code> に自動保存</li>
      <li>「Export JSON」で会話履歴を書き出し</li>
      <li>function calling: <code>get_weather</code> を実装済み</li>
    </ul>
  </main>

  <aside>
    <div class="panel-title">Transcript / Logs</div>
    <div class="log-wrap">
      <div class="log" id="log"></div>
      <div class="msg user" id="live-user" style="padding:12px; display:none; position: sticky; bottom: 48px; margin:0; border-bottom-left-radius:0; border-bottom-right-radius:0; border-color:#bfdbfe; background:#eff6ff;">
        <div class="role">USER <small>speaking...</small></div>
        <div class="text" id="live-user-text"></div>
      </div>
      <div class="msg assistant" id="live-assistant" style="padding:12px; display:none; position: sticky; bottom: 0; margin:0; border-top-left-radius: 0; border-top-right-radius:0; border-color:#d1fae5; background:#ecfdf5;">
        <div class="role">ASSISTANT <small>typing...</small></div>
        <div class="text" id="live-assistant-text"></div>
      </div>
    </div>
    <div class="footer">
      <small class="hint">Autosave: localStorage</small>
      <small id="ice" class="hint"></small>
    </div>
  </aside>

  <div id="avatar" class="avatar-wrap" title="Assistant Avatar" aria-hidden="true">
    <video id="avatarVideo" preload="auto" playsinline loop muted poster="avatar-poster.jpg">
      <source src="avatar-talking.mp4" type="video/mp4" />
    </video>
  </div>

  <script>
    // ========= DOM =========
    const btn = document.getElementById("toggle");
    const btnClear = document.getElementById("clear");
    const btnExport = document.getElementById("export");
    const statusEl = document.getElementById("status");
    const iceEl = document.getElementById("ice");
    const logEl = document.getElementById("log");
    const liveRow = document.getElementById("live-assistant");
    const liveTextEl = document.getElementById("live-assistant-text");
    const liveUserRow = document.getElementById("live-user");
    const liveUserTextEl = document.getElementById("live-user-text");
    const avatarEl = document.getElementById("avatar");
    const avatarVideoEl = document.getElementById("avatarVideo");

    // ========= Conversation log =========
    let conversation = [];
    function nowIso() { return new Date().toISOString(); }
    function escapeHtml(s) { return s.replace(/[&<>"']/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'}[c])); }
    function saveLocal() { try { localStorage.setItem("realtime_conversation", JSON.stringify(conversation)); } catch {} }
    function loadLocal() { try { const raw = localStorage.getItem("realtime_conversation"); if (raw) { conversation = JSON.parse(raw); renderLog(); } } catch {} }
    function clearLog() { conversation = []; saveLocal(); renderLog(); }
    function addMessage(role, text) { if (!text) return; conversation.push({ role, text, ts: nowIso() }); renderLog(); saveLocal(); }
    function renderLog() {
      logEl.innerHTML = "";
      for (const m of conversation) {
        const div = document.createElement("div");
        div.className = `msg ${m.role}`;
        div.innerHTML = `<div class="role">${m.role.toUpperCase()} <small>${m.ts}</small></div><div class="text">${escapeHtml(m.text)}</div>`;
        logEl.appendChild(div);
      }
      logEl.scrollTop = logEl.scrollHeight;
    }
    loadLocal();

    // ========= Avatar (VAD) =========
    let speakStopTimer = null;
    function startAvatarSpeaking() {
      if (speakStopTimer) { clearTimeout(speakStopTimer); speakStopTimer = null; }
      avatarEl.classList.add("speaking");
      try { if (avatarVideoEl.paused) { avatarVideoEl.currentTime = 0; avatarVideoEl.play(); } } catch (e) { console.warn("avatar play blocked:", e?.name || e); }
    }
    function stopAvatarSpeaking(immediate = false) {
      const stop = () => { avatarEl.classList.remove("speaking"); try { avatarVideoEl.pause(); } catch {} };
      if (immediate) return stop();
      if (speakStopTimer) clearTimeout(speakStopTimer);
      speakStopTimer = setTimeout(stop, 700);
    }

    let audioCtx = null, analyser = null, vadSrc = null, vadInterval = null, lastVoiceMs = 0;
    const VAD = { threshold: 0.018, hangoverMs: 900, intervalMs: 66 };

    function setupAssistantVAD(stream) {
      try {
        if (!audioCtx) {
          const AC = window.AudioContext || window.webkitAudioContext;
          audioCtx = new AC();
        }
        audioCtx.resume?.();

        if (vadSrc) { try { vadSrc.disconnect(); } catch {} vadSrc = null; }
        if (!analyser) {
          analyser = audioCtx.createAnalyser();
          analyser.fftSize = 2048;
          analyser.smoothingTimeConstant = 0.08;
        }
        vadSrc = audioCtx.createMediaStreamSource(stream);
        vadSrc.connect(analyser);

        const data = new Uint8Array(analyser.fftSize);
        lastVoiceMs = performance.now();
        if (vadInterval) { clearInterval(vadInterval); vadInterval = null; }

        vadInterval = setInterval(() => {
          analyser.getByteTimeDomainData(data);
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            const v = (data[i] - 128) / 128;
            sum += Math.abs(v);
          }
          const level = sum / data.length;
          const now = performance.now();

          if (level > VAD.threshold) {
            lastVoiceMs = now;
            startAvatarSpeaking();
          } else if (now - lastVoiceMs > VAD.hangoverMs) {
            stopAvatarSpeaking(true);
          }
        }, VAD.intervalMs);
      } catch (err) {
        console.warn("VAD setup failed:", err?.message || err);
      }
    }

    function teardownVAD() {
      if (vadInterval) { clearInterval(vadInterval); vadInterval = null; }
      if (vadSrc) { try { vadSrc.disconnect(); } catch {} vadSrc = null; }
    }

    // ========= Mic feature extraction (RMS / F0 / speech rate) =========
    let micSrc = null, micAnalyser = null, micInterval = null;
    const micFrames = []; // { ts, rms, f0 }
    const USER_DELTA_HISTORY = []; // { ts, len }
    let lastFinalUserText = "";

    function computeRMS(floatBuf) {
      let sum = 0;
      for (let i = 0; i < floatBuf.length; i++) {
        const v = floatBuf[i] || 0;
        sum += v * v;
      }
      return Math.sqrt(sum / (floatBuf.length || 1));
    }

    function estimateF0ACF(floatBuf, sampleRate) {
      // naive autocorrelation pitch estimator in 50..400 Hz
      const minF = 50, maxF = 400;
      const minLag = Math.floor(sampleRate / maxF);
      const maxLag = Math.floor(sampleRate / minF);
      const size = floatBuf.length;
      if (!size || maxLag >= size) return 0;
      // remove DC
      let mean = 0;
      for (let i = 0; i < size; i++) mean += floatBuf[i];
      mean /= size;
      const buf = new Float32Array(size);
      for (let i = 0; i < size; i++) buf[i] = floatBuf[i] - mean;
      // compute ACF
      let bestLag = 0, bestVal = 0;
      for (let lag = minLag; lag <= maxLag; lag++) {
        let sum = 0;
        for (let i = 0; i < size - lag; i++) sum += buf[i] * buf[i + lag];
        if (sum > bestVal) { bestVal = sum; bestLag = lag; }
      }
      if (bestLag <= 0) return 0;
      const f0 = sampleRate / bestLag;
      return (f0 >= minF && f0 <= maxF) ? f0 : 0;
    }

    function setupMicAnalysis(stream) {
      try {
        if (!audioCtx) {
          const AC = window.AudioContext || window.webkitAudioContext;
          audioCtx = new AC();
        }
        audioCtx.resume?.();
        if (micSrc) { try { micSrc.disconnect(); } catch {} micSrc = null; }
        if (!micAnalyser) {
          micAnalyser = audioCtx.createAnalyser();
          micAnalyser.fftSize = 2048;
          micAnalyser.smoothingTimeConstant = 0.02;
        }
        micSrc = audioCtx.createMediaStreamSource(stream);
        micSrc.connect(micAnalyser);

        const floatBuf = new Float32Array(micAnalyser.fftSize);
        const intervalMs = 100;
        if (micInterval) { clearInterval(micInterval); micInterval = null; }
        micInterval = setInterval(() => {
          try {
            micAnalyser.getFloatTimeDomainData(floatBuf);
            const rms = computeRMS(floatBuf);
            const f0 = estimateF0ACF(floatBuf, audioCtx.sampleRate || 44100);
            micFrames.push({ ts: performance.now(), rms, f0 });
            // keep last 10s
            const cutoff = performance.now() - 10_000;
            while (micFrames.length && micFrames[0].ts < cutoff) micFrames.shift();
            // also trim user delta history
            while (USER_DELTA_HISTORY.length && USER_DELTA_HISTORY[0].ts < cutoff) USER_DELTA_HISTORY.shift();
          } catch {}
        }, intervalMs);
      } catch (err) {
        console.warn("Mic analysis setup failed:", err?.message || err);
      }
    }

    function teardownMicAnalysis() {
      if (micInterval) { clearInterval(micInterval); micInterval = null; }
      if (micSrc) { try { micSrc.disconnect(); } catch {} micSrc = null; }
    }

    function getFeatureSummary(windowMs = 2000) {
      const now = performance.now();
      const from = now - windowMs;
      const frames = micFrames.filter(f => f.ts >= from);
      let f0_mean = 0, rms_mean = 0;
      if (frames.length) {
        for (const f of frames) { f0_mean += (f.f0 || 0); rms_mean += (f.rms || 0); }
        f0_mean /= frames.length;
        rms_mean /= frames.length;
      }
      // characters per second over the same window
      let chars = 0;
      for (const d of USER_DELTA_HISTORY) if (d.ts >= from) chars += (d.len || 0);
      const speech_rate = windowMs > 0 ? chars / (windowMs / 1000) : 0;
      return { f0_mean, rms_mean, speech_rate };
    }

    // ========= WebRTC session =========
    let pc = null, dc = null, micStream = null, remoteAudio = null;
    let running = false, cleaning = false;

    let currentAssistantText = "";
    let currentUserText = "";

    function setStatus(t) { statusEl.textContent = t; }
    function setRunningState(on) {
      running = on;
      btn.textContent = on ? "■ Stop" : "▶ Start";
      setStatus(on ? "Connected" : "Idle");
      if (!on) { iceEl.textContent = ""; }
    }

    async function fetchEphemeralSession() {
      const r = await fetch("/token", { method: "POST" });
      if (!r.ok) throw new Error(await r.text());
      return r.json();
    }

    // --- Function Calling state ---
    const toolCalls = new Map(); // call_id -> { name, argsText }

    async function startSession() {
      if (running) return;
      setStatus("Starting...");
      btn.disabled = true;
      try {
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        if (!audioCtx) {
          const AC = window.AudioContext || window.webkitAudioContext;
          audioCtx = new AC();
          await audioCtx.resume();
        }
        // Start mic feature extraction
        setupMicAnalysis(micStream);

        const session = await fetchEphemeralSession();
        pc = new RTCPeerConnection({
          iceServers: [{ urls: "stun:stun.l.google.com:19302" }]
        });
        micStream.getTracks().forEach(t => pc.addTrack(t, micStream));

        pc.ontrack = (e) => {
          if (remoteAudio) { try { remoteAudio.pause(); } catch {}; try { remoteAudio.remove(); } catch {} }
          remoteAudio = document.createElement("audio");
          remoteAudio.autoplay = true;
          remoteAudio.srcObject = e.streams[0];
          remoteAudio.addEventListener("play", () => startAvatarSpeaking());
          remoteAudio.addEventListener("pause", () => stopAvatarSpeaking(true));
          remoteAudio.addEventListener("ended", () => stopAvatarSpeaking(true));
          document.body.appendChild(remoteAudio);
          setupAssistantVAD(e.streams[0]);
        };

        const attachDataChannel = (channel) => {
          dc = channel;
          dc.onopen = () => {
            addMessage("event", "Session started");
            // 初期メッセージ（テスト兼ねる）。日本語・ツール使用の方針を明示。
            dc.send(JSON.stringify({
              type: "response.create",
              response: {
                instructions: "原則日本語で簡潔に回答。知識系の質問は必ず search_kb を1回呼び、上位ヒットを根拠に回答し、少なくとも1件の出典（タイトルとURL）を短く明示。天気は get_weather を使用。ユーザの各発話が確定するたびに analyze_emotion を必ず1回呼び、直近の音声特徴・テキストから感情(推定)を簡潔に要約し、語調を調整する。ヒット0件ならその旨と絞り込み提案。推測やハルシネーションは避ける。",
                modalities: ["text", "audio"],
                input_text: "テストです。まずは短く自己紹介し、続いて『東京の天気』の例と、『Function Calling や RAG とは？』の例を示してください。"
              }
            }));
          };

          let debugCount = 0;
          const DEBUG_LIMIT = 10;

          dc.onmessage = (ev) => {
            try {
              const e = JSON.parse(ev.data);
              if (debugCount < DEBUG_LIMIT) { console.log("[Realtime event]", e); debugCount++; }

              // ===== Assistant text delta =====
              let deltaText = "";
              if (e.type === "response.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (e.type === "response.text.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (e.type === "response.output_text.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (e.type === "response.audio_transcript.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (!deltaText && e?.delta?.text) deltaText = e.delta.text;

              if (deltaText) {
                currentAssistantText += deltaText;
                startAvatarSpeaking();
                // ★ ライブ字幕の表示・更新（修正点）
                liveRow.style.display = "block";
                liveTextEl.textContent = currentAssistantText;
                logEl.scrollTop = logEl.scrollHeight;
              }

              // ===== User transcript delta =====
              let userDelta = "";
              if (e.type === "conversation.item.input_audio_transcription.delta" && typeof e.delta === "string") userDelta = e.delta;
              if (!userDelta && e.type === "input_audio_buffer.transcript.delta" && typeof e.delta === "string") userDelta = e.delta;
              if (userDelta) {
                currentUserText += userDelta;
                liveUserRow.style.display = "block";
                liveUserTextEl.textContent = currentUserText;
                logEl.scrollTop = logEl.scrollHeight;
                USER_DELTA_HISTORY.push({ ts: performance.now(), len: userDelta.length });
              }

              // ===== Function Calling: arguments delta =====
              if (e.type === "response.function_call_arguments.delta") {
                const { call_id, delta, name } = e;
                const rec = toolCalls.get(call_id) || { name: name || "", argsText: "" };
                if (name && !rec.name) rec.name = name;
                rec.argsText += (delta || "");
                toolCalls.set(call_id, rec);
              }

              // ===== Function Calling: arguments done -> execute tool =====
              if (e.type === "response.function_call_arguments.done") {
                const { call_id, arguments: finalArgs, name } = e;
                const rec = toolCalls.get(call_id) || { name: name || "", argsText: finalArgs || "" };
                if (name && !rec.name) rec.name = name;

                let argsObj = {};
                try { argsObj = JSON.parse(rec.argsText || finalArgs || "{}"); } catch {}
                addMessage("event", `tool_call: ${rec.name || "unknown"} ${rec.argsText || ""}`);

                handleToolCall(rec.name, call_id, argsObj);
              }

              // ===== Assistant done =====
              const isDone =
                e.type === "response.completed" ||
                e.type === "response.text.done" ||
                e.type === "response.output_text.done" ||
                e.type === "response.audio_transcript.done";

              if (isDone) {
                const text = (liveTextEl.textContent || currentAssistantText).trim();
                if (text) addMessage("assistant", text);
                currentAssistantText = "";
                liveTextEl.textContent = "";
                liveRow.style.display = "none";
              }

              if (e.type === "response.done") {
                const text = (liveTextEl.textContent || currentAssistantText).trim();
                if (text) addMessage("assistant", text);
                currentAssistantText = "";
                liveTextEl.textContent = "";
                liveRow.style.display = "none";
              }

              // ===== User transcript done =====
              const userDone =
                e.type === "conversation.item.input_audio_transcription.completed" ||
                e.type === "input_audio_buffer.transcript.completed" ||
                e.type === "input_audio_buffer.transcript.done";

              if (userDone) {
                const finalUserText = (e?.transcript?.text || currentUserText || "").trim();
                if (finalUserText) addMessage("user", finalUserText);
                lastFinalUserText = finalUserText || lastFinalUserText;
                currentUserText = "";
                liveUserTextEl.textContent = "";
                liveUserRow.style.display = "none";

                // 毎発話で必ず analyze_emotion を要求する
                try {
                  if (finalUserText) {
                    const summary = getFeatureSummary(2500);
                    const args = {
                      f0_mean: Number(summary.f0_mean || 0),
                      rms_mean: Number(summary.rms_mean || 0),
                      speech_rate: Number(summary.speech_rate || 0),
                      transcript: finalUserText
                    };
                    const argsStr = JSON.stringify(args);
                    dc?.send(JSON.stringify({
                      type: "response.create",
                      response: {
                        instructions: `直前のユーザ発話に対して analyze_emotion ツールを必ず1回呼び出してください。引数は次のJSONをそのまま使ってください: ${argsStr}。ツールの出力({primary,valence,arousal,confidence})を短く要約し、それを踏まえて丁寧に応答してください。`,
                        modalities: ["text", "audio"],
                        input_text: "感情推定を実行してください。"
                      }
                    }));
                  }
                } catch (err) {
                  console.warn("force analyze_emotion failed:", err?.message || err);
                }
              }
            } catch { /* 非JSONは無視 */ }
          };
        };

        attachDataChannel(pc.createDataChannel("oai-events"));
        pc.ondatachannel = (ev) => { if (!dc || dc.readyState === "closed") attachDataChannel(ev.channel); };
        pc.oniceconnectionstatechange = () => { iceEl.textContent = `ICE: ${pc.iceConnectionState}`; };

        // SDP exchange
        const REALTIME_MODEL = new URLSearchParams(location.search).get("model") || "gpt-realtime";
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);

        const sdpUrl = `https://api.openai.com/v1/realtime?model=${encodeURIComponent(REALTIME_MODEL)}`;
        const resp = await fetch(sdpUrl, {
          method: "POST",
          headers: {
            "Authorization": `Bearer ${session.client_secret.value}`,
            "Content-Type": "application/sdp",
            "OpenAI-Beta": "realtime=v1"
          },
          body: offer.sdp
        });
        if (!resp.ok) {
          const t = await resp.text().catch(() => "");
          throw new Error(`Realtime answer error: ${resp.status} ${t}`);
        }
        const answerSdp = await resp.text();
        await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

        setRunningState(true);
      } catch (err) {
        console.error(err);
        setStatus("Failed to start: " + (err?.message || err));
        await stopSession(true);
      } finally {
        btn.disabled = false;
      }
    }

    async function stopSession(silent = false) {
      if (cleaning) return;
      cleaning = true;
      try {
        try { if (dc && dc.readyState !== "closed") dc.close(); } catch {}
        dc = null;

        if (pc) {
          try { pc.getSenders().forEach(s => { try { s.track.stop(); } catch {} }); } catch {}
          try { pc.close(); } catch {}
        }
        pc = null;

        if (micStream) { try { micStream.getTracks().forEach(t => t.stop()); } catch {} }
        micStream = null;

        if (remoteAudio) {
          try { remoteAudio.pause(); } catch {}
          try { remoteAudio.srcObject = null; } catch {}
          try { remoteAudio.remove(); } catch {}
        }
        remoteAudio = null;

        currentAssistantText = "";
        liveTextEl.textContent = "";
        liveRow.style.display = "none";
        currentUserText = "";
        liveUserTextEl.textContent = "";
        liveUserRow.style.display = "none";

        teardownVAD();
        teardownMicAnalysis();
        stopAvatarSpeaking(true);

        if (!silent) addMessage("event", "Session stopped");
      } finally {
        setRunningState(false);
        cleaning = false;
      }
    }

    // ========= Tool implementations =========
    function normalizeToolName(s) {
      try {
        return (s || "")
          .toString()
          .normalize("NFKC")
          .toLowerCase()
          .replace(/[\u200B\u200C\u200D]/g, "") // zero-width chars
          .trim()
          .replace(/^functions?\./, "")
          .replace(/[\s\-]+/g, "_");
      } catch { return (s || "").toString().trim().toLowerCase(); }
    }

    async function handleToolCall(name, callId, args) {
      try {
        let output = {};
        const toolName = normalizeToolName(name);
        // console.debug("tool name raw=", name, " normalized=", toolName);
        switch (toolName) {
          case "get_weather": {
            const { city = "東京", unit = "c" } = args || {};
            // 実運用ではバックエンドAPIや外部天気APIを呼ぶ
            // ここではダミー値（摂氏25℃ / 晴れ）
            output = {
              city,
              temperature: unit === "f" ? 77 : 25,
              condition: "晴れ",
              unit
            };
            break;
          }
          case "search_kb": {
            const { query = "", top_k = 5 } = args || {};
            const r = await fetch("/rag/search", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ query, top_k })
            });
            if (!r.ok) {
              const t = await r.text().catch(() => "");
              output = { error: `search_kb failed: ${r.status} ${t.slice(0,200)}` };
            } else {
              const data = await r.json();
              output = { results: data.results || [], query: data.query, top_k: data.top_k };
            }
            break;
          }
          case "analyze_emotion": {
            const summary = getFeatureSummary(2500);
            const payload = {
              f0_mean: Number.isFinite(args?.f0_mean) ? args.f0_mean : summary.f0_mean,
              rms_mean: Number.isFinite(args?.rms_mean) ? args.rms_mean : summary.rms_mean,
              speech_rate: Number.isFinite(args?.speech_rate) ? args.speech_rate : summary.speech_rate,
              transcript: (args?.transcript || lastFinalUserText || currentUserText || "").toString()
            };
            try {
              const r = await fetch("/emotion/analyze", {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify(payload)
              });
              if (!r.ok) {
                const t = await r.text().catch(() => "");
                output = { error: `analyze_emotion failed: ${r.status} ${t.slice(0,200)}` };
              } else {
                output = await r.json();
              }
            } catch (e) {
              output = { error: `analyze_emotion error: ${(e?.message || e)}` };
            }
            break;
          }
          default:
            if (toolName.includes("emotion")) {
              // Fallback mapping for variants like "analyze emotion", "functions.analyze-emotion" etc.
              const summary = getFeatureSummary(2500);
              const payload = {
                f0_mean: Number.isFinite(args?.f0_mean) ? args.f0_mean : summary.f0_mean,
                rms_mean: Number.isFinite(args?.rms_mean) ? args.rms_mean : summary.rms_mean,
                speech_rate: Number.isFinite(args?.speech_rate) ? args.speech_rate : summary.speech_rate,
                transcript: (args?.transcript || lastFinalUserText || currentUserText || "").toString()
              };
              try {
                const r = await fetch("/emotion/analyze", {
                  method: "POST",
                  headers: { "Content-Type": "application/json" },
                  body: JSON.stringify(payload)
                });
                if (!r.ok) {
                  const t = await r.text().catch(() => "");
                  output = { error: `analyze_emotion failed: ${r.status} ${t.slice(0,200)}` };
                } else {
                  output = await r.json();
                }
              } catch (e) {
                output = { error: `analyze_emotion error: ${(e?.message || e)}` };
              }
            } else {
              output = { error: `Unknown tool: ${name}` };
            }
        }

        // 1) function_call_output を会話に注入（call_id を必ず一致）
        dc?.send(JSON.stringify({
          type: "conversation.item.create",
          item: {
            type: "function_call_output",
            call_id: callId,
            output: JSON.stringify(output)
          }
        }));

        if (toolName === "analyze_emotion") {
          const fmt = (n, d = 2) => (Number.isFinite(Number(n)) ? Number(n).toFixed(d) : "-");
          const val = Number(output?.valence);
          const pol = String(output?.polarity || (Number.isFinite(val) ? (val > 0.15 ? "pos" : (val < -0.15 ? "neg" : "neu")) : "neu")).toUpperCase();
          const ar = Number(output?.arousal);
          const conf = Number(output?.confidence);
          const prim = output?.primary || "unknown";
          const f0 = Number(output?.features?.f0_mean);
          const rms = Number(output?.features?.rms_mean);
          const rate = Number(output?.features?.speech_rate);
          addMessage("event", `emotion: [${pol}] ${prim} | v=${fmt(val)} a=${fmt(ar)} c=${fmt(conf)} | f0=${fmt(f0)}Hz rms=${fmt(rms,3)} rate=${fmt(rate)}/s`);
        }

        addMessage("event", `tool_output: ${name} -> ${JSON.stringify(output)}`);

        // 2) 結果を踏まえて応答を生成
        dc?.send(JSON.stringify({
          type: "response.create",
          response: {
            instructions: "直前の関数結果に基づき、日本語で簡潔に回答。search_kb の結果が含まれる場合は最低1件の出典（タイトルとURL）を短く明示。ヒット0件のときはその旨と追加の質問/絞り込み提案を行う。推測やハルシネーションは避ける。",
            modalities: ["text", "audio"]
          }
        }));
      } catch (err) {
        console.error("tool error:", err);
      }
    }

    // ========= UI =========
    btn.addEventListener("click", async () => {
      if (running) { btn.disabled = true; await stopSession(); btn.disabled = false; }
      else { await startSession(); }
    });
    btnClear.addEventListener("click", () => { if (confirm("ログを消去しますか？")) clearLog(); });
    btnExport.addEventListener("click", () => {
      const blob = new Blob([JSON.stringify({ conversation }, null, 2)], { type: "application/json" });
      const a = document.createElement("a");
      const ts = new Date().toISOString().replace(/[:.]/g, "");
      a.href = URL.createObjectURL(blob);
      a.download = `conversation-${ts}.json`;
      a.click();
      URL.revokeObjectURL(a.href);
    });
    window.addEventListener("beforeunload", () => {
      try { stopAvatarSpeaking(true); } catch {}
      try { teardownVAD(); } catch {}
    });
  </script>
</body>
</html>
