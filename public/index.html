<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <title>OpenAI Realtime (WebRTC) - Toggle & Transcript & Save</title>
  <style>
    :root { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
    * { box-sizing: border-box; }
    body { margin: 0; display: grid; grid-template-columns: 1fr 420px; grid-template-rows: auto 1fr; height: 100vh; }
    header { grid-column: 1 / -1; padding: 10px 16px; background: #111827; color: #e5e7eb; display: flex; gap: 12px; align-items: center; }
    main { padding: 16px; overflow: auto; }
    aside { border-left: 1px solid #e5e7eb; display: grid; grid-template-rows: auto 1fr auto; min-width: 320px; }
    .controls { display: flex; gap: 8px; align-items: center; }
    button { padding: 8px 14px; font-size: 14px; cursor: pointer; border: 1px solid #ddd; border-radius: 8px; background: #fff; }
    button:disabled { opacity: .6; cursor: not-allowed; }
    .status { margin-left: auto; opacity: .9; font-size: 14px; }
    .panel-title { padding: 10px 12px; font-weight: 600; border-bottom: 1px solid #eee; background: #f8fafc; }
    .log-wrap { position: relative; }
    .log { overflow: auto; padding: 12px; background: #fafafa; border-top: 1px solid #eee; border-bottom: 1px solid #eee; height: 100%; }
    .msg { margin: 10px 0; border-radius: 8px; padding: 8px 10px; background: #fff; border: 1px solid #eee; }
    .role { font-weight: 700; font-size: 12px; opacity: .75; margin-bottom: 4px; }
    .user .role { color: #2563eb; }
    .assistant .role { color: #059669; }
    .event .role { color: #6b7280; }
    .text { white-space: pre-wrap; word-break: break-word; }
    .footer { padding: 10px; display: flex; gap: 8px; justify-content: space-between; align-items: center; }
    .hint { font-size: 12px; color: #6b7280; }

    /* Avatar: 左サイドに大きく常時表示（停止はpause） */
    .avatar-wrap {
      position: fixed; left: 16px; top: 50%; transform: translateY(-50%);
      width: 360px; height: 360px;
      border-radius: 9999px; overflow: hidden; border: 2px solid #e5e7eb; background: #000; z-index: 20;
      box-shadow: 0 10px 30px rgba(0,0,0,.25);
      pointer-events: none; /* UI操作の邪魔をしない */
    }
    .avatar-wrap video { width: 100%; height: 100%; object-fit: cover; }
    .avatar-wrap.speaking { animation: avatarPulse 1.4s infinite; }
    @keyframes avatarPulse {
      0% { box-shadow: 0 0 0 0 rgba(99,102,241,.6); }
      70% { box-shadow: 0 0 0 28px rgba(99,102,241,0); }
      100% { box-shadow: 0 0 0 0 rgba(99,102,241,0); }
    }
    @media (max-width: 1200px) {
      .avatar-wrap { width: 260px; height: 260px; }
    }
  </style>
</head>
<body>
  <header>
    <div class="controls">
      <button id="toggle">▶ Start</button>
      <button id="clear" title="ログを消去">🧹 Clear</button>
      <button id="export" title="JSONで保存">💾 Export JSON</button>
    </div>
    <div class="status" id="status">Idle</div>
  </header>

  <main>
    <h2>Realtime Voice Demo</h2>
    <p>
      「▶ Start」で会話開始／「■ Stop」で停止。<br/>
      右側にアシスタントの返答が <b>字幕としてリアルタイム表示</b> され、確定後はログに保存されます。
    </p>
    <ul>
      <li>ログは <code>localStorage</code> に自動保存（リロードしても残ります）</li>
      <li>「Export JSON」で会話履歴を書き出し</li>
      <li>ユーザー字幕は現時点ではオフ（後から追加可能）</li>
    </ul>
  </main>

  <aside>
    <div class="panel-title">Transcript / Logs</div>

    <div class="log-wrap">
      <div class="log" id="log"></div>

      <!-- ライブ字幕（ユーザー） -->
      <div class="msg user" id="live-user" style="padding:12px; display:none; position: sticky; bottom: 48px; margin:0; border-bottom-left-radius:0; border-bottom-right-radius:0; border-color:#bfdbfe; background:#eff6ff;">
        <div class="role">USER <small>speaking...</small></div>
        <div class="text" id="live-user-text"></div>
      </div>

      <!-- ライブ字幕（アシスタント） -->
      <div class="msg assistant" id="live-assistant" style="padding:12px; display:none; position: sticky; bottom: 0; margin:0; border-top-left-radius: 0; border-top-right-radius:0; border-color:#d1fae5; background:#ecfdf5;">
        <div class="role">ASSISTANT <small>typing...</small></div>
        <div class="text" id="live-assistant-text"></div>
      </div>
    </div>

    <div class="footer">
      <small class="hint">Autosave: localStorage</small>
      <small id="ice" class="hint"></small>
    </div>
  </aside>

  <!-- Assistant Avatar: 左に常時表示（停止はpause） -->
  <div id="avatar" class="avatar-wrap" title="Assistant Avatar" aria-hidden="true">
    <video id="avatarVideo" preload="auto" playsinline loop muted poster="avatar-poster.jpg">
      <source src="avatar-talking.mp4" type="video/mp4" />
    </video>
  </div>

  <script>
    // ========= DOM =========
    const btn = document.getElementById("toggle");
    const btnClear = document.getElementById("clear");
    const btnExport = document.getElementById("export");
    const statusEl = document.getElementById("status");
    const iceEl = document.getElementById("ice");
    const logEl = document.getElementById("log");
    const liveRow = document.getElementById("live-assistant");
    const liveTextEl = document.getElementById("live-assistant-text");
    const liveUserRow = document.getElementById("live-user");
    const liveUserTextEl = document.getElementById("live-user-text");
    // アバター
    const avatarEl = document.getElementById("avatar");
    const avatarVideoEl = document.getElementById("avatarVideo");

    // ========= 会話ログ（localStorage連携） =========
    let conversation = [];
    function nowIso() { return new Date().toISOString(); }
    function escapeHtml(s) { return s.replace(/[&<>"']/g, c => ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'}[c])); }
    function saveLocal() { try { localStorage.setItem("realtime_conversation", JSON.stringify(conversation)); } catch {} }
    function loadLocal() { try { const raw = localStorage.getItem("realtime_conversation"); if (raw) { conversation = JSON.parse(raw); renderLog(); } } catch {} }
    function clearLog() { conversation = []; saveLocal(); renderLog(); }
    function addMessage(role, text) { if (!text) return; conversation.push({ role, text, ts: nowIso() }); renderLog(); saveLocal(); }
    function renderLog() {
      logEl.innerHTML = "";
      for (const m of conversation) {
        const div = document.createElement("div");
        div.className = `msg ${m.role}`;
        div.innerHTML = `<div class="role">${m.role.toUpperCase()} <small>${m.ts}</small></div><div class="text">${escapeHtml(m.text)}</div>`;
        logEl.appendChild(div);
      }
      logEl.scrollTop = logEl.scrollHeight;
    }
    loadLocal();

    // ========= Avatar Control（VADで制御：音が出ている間は再生） =========
    let speakStopTimer = null;
    function startAvatarSpeaking() {
      if (speakStopTimer) { clearTimeout(speakStopTimer); speakStopTimer = null; }
      avatarEl.classList.add("speaking");
      try {
        if (avatarVideoEl.paused) {
          avatarVideoEl.currentTime = 0;
          avatarVideoEl.play();
        }
      } catch (e) { console.warn("avatar play blocked:", e?.name || e); }
    }
    function stopAvatarSpeaking(immediate = false) {
      const stop = () => {
        avatarEl.classList.remove("speaking");
        try { avatarVideoEl.pause(); } catch {}
      };
      if (immediate) return stop();
      if (speakStopTimer) clearTimeout(speakStopTimer);
      speakStopTimer = setTimeout(stop, 700); // 余韻
    }

    // ====== Web Audio VAD（音声エネルギー監視） ======
    let audioCtx = null, analyser = null, vadSrc = null, vadInterval = null, lastVoiceMs = 0;
    const VAD = { threshold: 0.018, hangoverMs: 900, intervalMs: 66 }; // 目安: 0.015〜0.03

    function setupAssistantVAD(stream) {
      try {
        if (!audioCtx) {
          const AC = window.AudioContext || window.webkitAudioContext;
          audioCtx = new AC();
        }
        audioCtx.resume?.();

        if (vadSrc) { try { vadSrc.disconnect(); } catch {} vadSrc = null; }
        if (!analyser) {
          analyser = audioCtx.createAnalyser();
          analyser.fftSize = 2048;
          analyser.smoothingTimeConstant = 0.08;
        }
        vadSrc = audioCtx.createMediaStreamSource(stream);
        vadSrc.connect(analyser);

        const data = new Uint8Array(analyser.fftSize);
        lastVoiceMs = performance.now();
        if (vadInterval) { clearInterval(vadInterval); vadInterval = null; }

        vadInterval = setInterval(() => {
          analyser.getByteTimeDomainData(data);
          // 平均絶対偏差でレベル推定（0..1）
          let sum = 0;
          for (let i = 0; i < data.length; i++) {
            const v = (data[i] - 128) / 128;
            sum += Math.abs(v);
          }
          const level = sum / data.length; // 小さければ無音に近い
          const now = performance.now();

          if (level > VAD.threshold) {
            lastVoiceMs = now;
            startAvatarSpeaking(); // 音が出ている限り再生維持
          } else if (now - lastVoiceMs > VAD.hangoverMs) {
            stopAvatarSpeaking(true); // 一定時間無音で停止
          }
        }, VAD.intervalMs);
      } catch (err) {
        console.warn("VAD setup failed:", err?.message || err);
      }
    }

    function teardownVAD() {
      if (vadInterval) { clearInterval(vadInterval); vadInterval = null; }
      if (vadSrc) { try { vadSrc.disconnect(); } catch {} vadSrc = null; }
      // analyser / audioCtx は再利用可。閉じたい場合は以下を有効化。
      // try { audioCtx?.close(); } catch {}
    }

    // ========= 接続制御 =========
    let pc = null, dc = null, micStream = null, remoteAudio = null;
    let running = false, cleaning = false;

    let currentAssistantText = "";
    let currentUserText = "";

    function setStatus(t) { statusEl.textContent = t; }
    function setRunningState(on) {
      running = on;
      btn.textContent = on ? "■ Stop" : "▶ Start";
      setStatus(on ? "Connected" : "Idle");
      if (!on) { iceEl.textContent = ""; }
    }

    async function fetchEphemeralSession() {
      const r = await fetch("/token", { method: "POST" });
      if (!r.ok) throw new Error(await r.text());
      return r.json();
    }

    async function startSession() {
      if (running) return;
      setStatus("Starting...");
      btn.disabled = true;
      try {
        // 1) マイク（ユーザー操作内でAudioContextを起こせる）
        micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        // AudioContextは早めに起動（iOS対策）
        if (!audioCtx) {
          const AC = window.AudioContext || window.webkitAudioContext;
          audioCtx = new AC();
          await audioCtx.resume();
        }

        // 2) Ephemeral token
        const session = await fetchEphemeralSession();

        // 3) PeerConnection
        pc = new RTCPeerConnection();
        micStream.getTracks().forEach(t => pc.addTrack(t, micStream));

        // 4) 受信音声
        pc.ontrack = (e) => {
          if (remoteAudio) { try { remoteAudio.pause(); } catch {}; try { remoteAudio.remove(); } catch {} }
          remoteAudio = document.createElement("audio");
          remoteAudio.autoplay = true;
          remoteAudio.srcObject = e.streams[0];
          // 受信オーディオイベント（補強）
          remoteAudio.addEventListener("play", () => startAvatarSpeaking());
          remoteAudio.addEventListener("pause", () => stopAvatarSpeaking(true));
          remoteAudio.addEventListener("ended", () => stopAvatarSpeaking(true));
          document.body.appendChild(remoteAudio);

          // ★ Assistant音声のVADを開始
          setupAssistantVAD(e.streams[0]);
        };

        // 5) DataChannel
        const attachDataChannel = (channel) => {
          dc = channel;
          dc.onopen = () => {
            addMessage("event", "Session started");
            dc.send(JSON.stringify({
              type: "response.create",
              response: {
                instructions: "あなたは親切な音声アシスタント。簡潔な日本語で要点を返答してください。",
                modalities: ["text", "audio"]
              }
            }));
            dc.send(JSON.stringify({
              type: "response.create",
              response: {
                input_text: "テストです。短く自己紹介してみてください。",
                modalities: ["text", "audio"]
              }
            }));
          };

          let debugCount = 0;
          const DEBUG_LIMIT = 10;

          dc.onmessage = (ev) => {
            try {
              const e = JSON.parse(ev.data);
              if (debugCount < DEBUG_LIMIT) { console.log("[Realtime event]", e); debugCount++; }

              // ===== アシスタントのテキスト増分（開始トリガのみ利用）=====
              let deltaText = "";
              if (e.type === "response.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (e.type === "response.text.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (e.type === "response.output_text.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (e.type === "response.audio_transcript.delta" && typeof e.delta === "string") deltaText = e.delta;
              if (!deltaText && e?.delta?.text) deltaText = e.delta.text;

              if (deltaText) {
                currentAssistantText += deltaText;
                // 開始は積極的に（VADと二重でもOK）
                startAvatarSpeaking();
              }

              // ===== ユーザー音声の文字起こし（delta）=====
              let userDelta = "";
              if (e.type === "conversation.item.input_audio_transcription.delta" && typeof e.delta === "string") userDelta = e.delta;
              if (!userDelta && e.type === "input_audio_buffer.transcript.delta" && typeof e.delta === "string") userDelta = e.delta;

              if (userDelta) {
                currentUserText += userDelta;
                liveUserRow.style.display = "block";
                liveUserTextEl.textContent = currentUserText;
                logEl.scrollTop = logEl.scrollHeight;
              }

              // ===== 応答完了 =====
              const isDone =
                e.type === "response.completed" ||
                e.type === "response.text.done" ||
                e.type === "response.output_text.done" ||
                e.type === "response.audio_transcript.done";

              if (isDone) {
                const text = currentAssistantText.trim();
                if (text) addMessage("assistant", text);
                currentAssistantText = "";
                liveTextEl.textContent = "";
                liveRow.style.display = "none";
                // ★ ここで stop はしない → VAD（実際の無音）で止める
                // stopAvatarSpeaking();  // ← 無効化
              }

              // 公式：response.done も同様
              if (e.type === "response.done") {
                const text = currentAssistantText.trim();
                if (text) addMessage("assistant", text);
                currentAssistantText = "";
                liveTextEl.textContent = "";
                liveRow.style.display = "none";
                // stopAvatarSpeaking(); // ← 無効化
              }

              // ===== ユーザー側の完了 =====
              const userDone =
                e.type === "conversation.item.input_audio_transcription.completed" ||
                e.type === "input_audio_buffer.transcript.completed" ||
                e.type === "input_audio_buffer.transcript.done";

              if (userDone) {
                const finalUserText = (e?.transcript?.text || currentUserText || "").trim();
                if (finalUserText) addMessage("user", finalUserText);
                currentUserText = "";
                liveUserTextEl.textContent = "";
                liveUserRow.style.display = "none";
              }
            } catch { /* 非JSONは無視 */ }
          };
        };

        attachDataChannel(pc.createDataChannel("oai-events"));
        pc.ondatachannel = (ev) => { if (!dc || dc.readyState === "closed") attachDataChannel(ev.channel); };

        pc.oniceconnectionstatechange = () => {
          const s = pc.iceConnectionState;
          iceEl.textContent = `ICE: ${s}`;
        };

        // 6) SDP交換
        const offer = await pc.createOffer({ offerToReceiveAudio: true });
        await pc.setLocalDescription(offer);

        const sdpUrl = "https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview";
        const resp = await fetch(sdpUrl, {
          method: "POST",
          headers: {
            "Authorization": `Bearer ${session.client_secret.value}`,
            "Content-Type": "application/sdp",
            "OpenAI-Beta": "realtime=v1"
          },
          body: offer.sdp
        });
        const answerSdp = await resp.text();
        await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

        setRunningState(true);
      } catch (err) {
        console.error(err);
        setStatus("Failed to start: " + (err?.message || err));
        await stopSession(true);
      } finally {
        btn.disabled = false;
      }
    }

    async function stopSession(silent = false) {
      if (cleaning) return;
      cleaning = true;
      try {
        try { if (dc && dc.readyState !== "closed") dc.close(); } catch {}
        dc = null;

        if (pc) {
          try { pc.getSenders().forEach(s => { try { s.track.stop(); } catch {} }); } catch {}
          try { pc.close(); } catch {}
        }
        pc = null;

        if (micStream) {
          try { micStream.getTracks().forEach(t => t.stop()); } catch {}
        }
        micStream = null;

        if (remoteAudio) {
          try { remoteAudio.pause(); } catch {}
          try { remoteAudio.srcObject = null; } catch {}
          try { remoteAudio.remove(); } catch {}
        }
        remoteAudio = null;

        // ライブ行リセット
        currentAssistantText = "";
        liveTextEl.textContent = "";
        liveRow.style.display = "none";
        liveUserTextEl.textContent = "";
        liveUserRow.style.display = "none";

        // VAD停止 & アバター停止
        teardownVAD();
        stopAvatarSpeaking(true);

        if (!silent) addMessage("event", "Session stopped");
      } finally {
        setRunningState(false);
        cleaning = false;
      }
    }

    // ========= UIイベント =========
    btn.addEventListener("click", async () => {
      if (running) { btn.disabled = true; await stopSession(); btn.disabled = false; }
      else { await startSession(); }
    });
    btnClear.addEventListener("click", () => { if (confirm("ログを消去しますか？")) clearLog(); });
    btnExport.addEventListener("click", () => {
      const blob = new Blob([JSON.stringify({ conversation }, null, 2)], { type: "application/json" });
      const a = document.createElement("a");
      const ts = new Date().toISOString().replace(/[:.]/g, "");
      a.href = URL.createObjectURL(blob);
      a.download = `conversation-${ts}.json`;
      a.click();
      URL.revokeObjectURL(a.href);
    });
    window.addEventListener("beforeunload", () => {
      try { stopAvatarSpeaking(true); } catch {}
      try { teardownVAD(); } catch {}
    });
  </script>
</body>
</html>
